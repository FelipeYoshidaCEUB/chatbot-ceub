"""
Chatbot RAG com m√∫ltiplos PDFs.
‚Äì Responde perguntas com base nos documentos
‚Äì Cita a fonte [Fonte: nome_do_pdf.pdf p.X] apenas se usar
‚Äì Ignora cita√ß√µes quando n√£o for necess√°rio
"""

# Mais pra frente adicionar o conversation buffer memory


import os
import warnings
from pathlib import Path
from dotenv import load_dotenv

from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import HumanMessage, AIMessage

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import ChatPromptTemplate
from langchain_core.prompts import PromptTemplate

# ---------------------------------------------------------------
# 0. Configura√ß√µes
# ---------------------------------------------------------------
load_dotenv()
warnings.filterwarnings("ignore", category=UserWarning)

PDF_DIR = Path("data/")
EMBED_MODEL = "text-embedding-3-small"
CHAT_MODEL = "gpt-4o-mini"
CHUNK_SIZE = 1500
CHUNK_OVERLAP = 200

SYSTEM_PROMPT = (
    "Voc√™ √© um assistente especializado em parto, pr√©-natal, enfermagem e na empresa de enfermagem. "
    "Use exclusivamente o conte√∫do dos documentos fornecidos para responder perguntas sobre os temas. "
    "Caso o contexto n√£o contenha informa√ß√µes relevantes √† pergunta, informe isso de forma clara e respeitosa ao usu√°rio. Nunca forne√ßa informa√ß√µes que n√£o estejam no conte√∫do fornecido, e jamais invente ou suponha dados."
    "Utilize o conte√∫do com precis√£o. Sempre que utilizar alguma informa√ß√£o do contexto, cite separadamente a fonte correspondente. Ao final de cada resposta, apresente uma lista individualizada com o nome dos arquivos ou refer√™ncias utilizadas, sob o t√≠tulo: Fontes consultadas."
    "Se a informa√ß√£o n√£o estiver nos documentos, diga exatamente: "
    "\"N√£o encontrei essa informa√ß√£o no material fornecido.\" "
    "Adote um tom profissional e objetivo. Explique termos t√©cnicos de forma acess√≠vel, sempre que necess√°rio, mantendo clareza e precis√£o nas respostas."
)

# ---------------------------------------------------------------
# 1. Carregar todos os PDFs da pasta / dividir em chunks
# ---------------------------------------------------------------
all_chunks = []

for pdf_file in PDF_DIR.glob("*.pdf"):
    loader = PyPDFLoader(str(pdf_file))
    docs = loader.load()

    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    chunks = splitter.split_documents(docs)

    for d in chunks:
        page = d.metadata.get("page", 0) + 1
        d.metadata["source"] = f"{pdf_file.name} p.{page}"

    all_chunks.extend(chunks)

# ---------------------------------------------------------------
# 2. Gerar embeddings e indexar no FAISS
# ---------------------------------------------------------------
embeddings = OpenAIEmbeddings(model=EMBED_MODEL)
vectordb = FAISS.from_documents(all_chunks, embeddings)

# ---------------------------------------------------------------
# 3. Prompt + Cadeia de Pergunta-Resposta
# ---------------------------------------------------------------
document_prompt = PromptTemplate.from_template(
    "{page_content}\n\n[Fonte: {source}]"
)

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", SYSTEM_PROMPT),
    ("human",
     "Pergunta: {question}\n\n"
     "Contexto extra√≠do dos documentos:\n{context}\n\n"
     "Resposta detalhada:")
])

llm = ChatOpenAI(model_name=CHAT_MODEL, temperature=0.2)

retriever = vectordb.as_retriever(
    search_kwargs={"k": 4}
)

qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    combine_docs_chain_kwargs={
        "prompt": chat_prompt,
        "document_variable_name": "context",
        "document_prompt": document_prompt,
    },
    return_source_documents=True,
)

# ---------------------------------------------------------------
# 4. Loop de Conversa no Terminal
# ---------------------------------------------------------------
chat_history = []

print("‚úÖ Chatbot RAG pronto! (Digite 'sair' para encerrar)\n")

while True:
    pergunta = input("Voc√™: ")
    if pergunta.lower() in {"sair", "exit", "quit"}:
        print("üëã At√© logo!")
        break

    chat_history.append(HumanMessage(content=pergunta))

    result = qa_chain.invoke({"question": pergunta, "chat_history": chat_history})
    resposta = result["answer"].strip()

    chat_history.append(AIMessage(content=resposta))

    print("\nAssistente:", resposta, "\n")

